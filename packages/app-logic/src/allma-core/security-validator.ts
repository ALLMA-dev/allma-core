import { XMLParser } from 'fast-xml-parser';
import {
  type SecurityValidatorConfig,
  SecurityViolationError,
} from '@allma/core-types';
import { log_info, log_warn, log_error } from '@allma/core-sdk';
import { getVertexAIEmbeddingsClient } from './utils/vertexAiClient';


// Helper for cosine similarity
function cosineSimilarity(vecA: number[], vecB: number[]): number {
    // Ensure both vectors are non-empty and of the same length
    if (vecA.length !== vecB.length || vecA.length === 0) return 0;
    let dotProduct = 0, magA = 0, magB = 0;
    for (let i = 0; i < vecA.length; i++) {
        dotProduct += vecA[i] * vecB[i];
        magA += vecA[i] * vecA[i];
        magB += vecB[i] * vecB[i];
    }
    magA = Math.sqrt(magA);
    magB = Math.sqrt(magB);
    if (magA === 0 || magB === 0) return 0;
    return dotProduct / (magA * magB);
}

/**
 * Validates an LLM's output against a security configuration.
 * @param llmResponseText The text generated by the LLM.
 * @param fullPromptContent The complete content of the prompt used to generate the response.
 * @param config The security configuration from the step definition.
 * @param correlationId For logging.
 * @throws {SecurityViolationError} if any check fails.
 */
export async function validateLlmOutput(
  llmResponseText: string,
  fullPromptContent: string,
  config: SecurityValidatorConfig,
  correlationId: string,
): Promise<void> {
  const answerText = llmResponseText.toLowerCase();

  // 1. Forbidden String Check
  if (config.forbiddenStrings) {
    for (const forbidden of config.forbiddenStrings) {
      if (typeof forbidden !== 'string') {
        log_warn(
          'Security Validator: non-string value found in forbiddenStrings array. Skipping this entry.',
          { value: forbidden, type: typeof forbidden },
          correlationId
        );
        continue; // Skip to the next item in the array
      }
      
      if (answerText.includes(forbidden.toLowerCase())) {
        throw new SecurityViolationError(`Output contained forbidden string: "${forbidden}"`);
      }
    }
  }

  // 2. Semantic Similarity Check
  if (config.semanticCheck) {
    try {
      const { similarityThreshold } = config.semanticCheck;
      
      // We must get the protected content from the prompt itself.
      // We'll parse the XML and find the 'protected_content_source' as defined in our ChatBot example.
      // A more robust solution might define a standard tag like <protect>...</protect>
      const xmlParser = new XMLParser();
      const parsedXml = xmlParser.parse(fullPromptContent);
      const protectedContent = parsedXml?.prompt_template?.system_configuration?.security_configuration?.semantic_check?.protected_content_source;

      if (!protectedContent) {
          log_warn("Semantic check configured, but no <protected_content_source> tag found in prompt. Skipping check.", {}, correlationId);
          return;
      }
      
      const embeddingsClient = await getVertexAIEmbeddingsClient(
          process.env.AI_API_KEYS_SECRET_ARN!, // Assuming a generic env var name
          process.env.GOOGLE_VERTEX_AI_LOCATION!,
          process.env.GOOGLE_VERTEX_AI_MODEL!,
          correlationId
      );

      const [promptEmbedding, answerEmbedding] = await Promise.all([
        embeddingsClient.embedQuery(protectedContent),
        embeddingsClient.embedQuery(answerText)
      ]);

      const similarity = cosineSimilarity(promptEmbedding, answerEmbedding);
      log_info(`Semantic similarity score: ${similarity.toFixed(4)}`, { threshold: similarityThreshold }, correlationId);

      if (similarity > similarityThreshold) {
        throw new SecurityViolationError(`Semantic similarity of ${similarity.toFixed(4)} exceeded threshold of ${similarityThreshold}.`);
      }

    } catch (e: any) {
        if (e instanceof SecurityViolationError) throw e; // Re-throw our specific error
        log_error("Error during semantic similarity check. Failing open (allowing response).", { error: e.message }, correlationId);
        // Do not throw for infrastructure errors, fail open.
    }
  }

  log_info("LLM output passed all security checks.", {}, correlationId);
}