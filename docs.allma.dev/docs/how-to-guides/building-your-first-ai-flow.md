---
title: Building Your First AI Flow
sidebar_position: 1
---

# Building Your First AI Flow

This guide will walk you through creating a simple but powerful Allma Flow: taking a piece of text as input, using a Large Language Model (LLM) to summarize it, and storing the summary.

### Goal

To create a flow that accepts a JSON payload like `{"articleText": "..."}` and produces a summary.

### Prerequisites

1.  **An Allma instance deployed.** (See the [Quick Start](./../getting-started/quick-start.md) guide).
2.  **A Prompt Template.** Before building the flow, create a Prompt Template in the Allma Admin Panel.
    -   **Name:** `Summarize Text`
    -   **Content:**
        ```handlebars
        Please summarize the following article in two sentences:

        --- ARTICLE START ---
        {{text_to_summarize}}
        --- ARTICLE END ---
        ```

### Step 1: Create a New Flow

1.  Navigate to the **Flows** section in the Allma Admin Panel.
2.  Click **"Create Flow"**.
3.  Give it a name, like `Article Summarizer`, and click **Create**.
4.  You will be taken to the Flow Editor canvas with a single "Start" step.

### Step 2: Add and Configure the LLM Step

1.  From the step library on the left, drag an **LLM Invocation** step onto the canvas.
2.  Connect the "Start" step to your new LLM step by dragging from the output handle of "Start" to the input handle of the LLM step.
3.  Select the LLM step to open its configuration panel.

#### Configure the LLM Invocation:

1.  **Display Name:** Name it something descriptive, like `Generate Summary`.
2.  **LLM Provider:** Select your configured provider (e.g., `GEMINI` or `AWS_BEDROCK`).
3.  **Model ID:** Choose a suitable model (e.g., `gemini-1.5-pro-latest`).
4.  **Prompt Template ID:** Select the `Summarize Text` prompt template you created earlier.
5.  **Inference Parameters:** You can leave these as default for now.

### Step 3: Map Data Into the Step

We need to tell the LLM step where to get the article text from. This is done with **Input Mappings**. The `Summarize Text` prompt expects a variable named `text_to_summarize`.

1.  In the LLM step's configuration, go to the **Input Mappings** section.
2.  Add a new mapping:
    -   **Target Path (Key):** `text_to_summarize`
    -   **Source JSONPath (Value):** `$.initialContextData.articleText`

This tells Allma: "When this step runs, take the value from `initialContextData.articleText` in the main flow context and make it available to the prompt template as the `text_to_summarize` variable."

### Step 4: Map Data Out of the Step

After the LLM generates a summary, we need to save it back to the flow context. This is done with **Output Mappings**.

1.  Go to the **Output Mappings** section.
2.  Add a new mapping:
    -   **Target Path (Key):** `$.steps_output.summary`
    -   **Source JSONPath (Value):** `$.`

This tells Allma: "Take the entire output object from this step (which will look like `{"llm_response": "..."}`) and save it to `steps_output.summary` in the main flow context."

### Step 5: Publish and Trigger the Flow

1.  Click **"Save Draft"** and then **"Publish"** to make your flow live.
2.  Trigger your flow by sending a request to the Allma Flow Trigger API. You can use a tool like `curl` or Postman.

    ```bash title="Example using curl"
    curl -X POST 'https://YOUR_API_GATEWAY_URL/trigger/article-summarizer' \
    -H 'Content-Type: application/json' \
    -d '{
      "articleText": "Allma is a serverless, event-driven platform designed to build, execute, and manage complex, AI-powered automated workflows, known as Flows. It acts as a digital factory for orchestrating sophisticated business processes, combining data integration, conditional logic, and advanced AI capabilities in a robust, scalable, and observable environment."
    }'
    ```

You will receive a `flowExecutionId` in the response.

### Step 6: Verify the Execution

1.  Go to the **Executions** tab in the Admin Panel.
2.  Find the execution using the `flowExecutionId` from the API response.
3.  Click on it to view the detailed step-by-step log.
4.  Select the `Generate Summary` step and inspect its **Output Context**. You should see the summary generated by the LLM.
5.  Check the **Diff** view to see exactly how the `steps_output.summary` object was added to the flow context.

Congratulations! You've successfully built and executed your first AI-powered workflow with Allma.